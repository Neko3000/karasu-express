/**
 * Prompt Optimizer Service
 *
 * Transforms brief subject descriptions into detailed, high-quality prompts using AI (LLM).
 * Implements the intelligent prompt optimization workflow for User Story 2.
 *
 * Features:
 * - LLM-based prompt expansion using structured JSON output
 * - Multiple variant generation (Realistic, Artistic, Cinematic, etc.)
 * - Optional web search enhancement for trending topics (RAG)
 * - Provider abstraction for future ChatGPT/Claude support
 *
 * Primary LLM: Gemini Pro (Google AI)
 * Future expansion: ChatGPT (GPT-4o), Claude (Claude 3.5 Sonnet)
 */

import { GoogleGenerativeAI } from '@google/generative-ai'

// ============================================
// CONSTANTS
// ============================================

/**
 * Default number of prompt variants to generate
 */
export const DEFAULT_VARIANT_COUNT = 3

/**
 * System prompt for the prompt engineering expert persona
 * Instructs the LLM to act as a specialized prompt engineer for image generation
 */
export const SYSTEM_PROMPT = `You are an expert prompt engineer specializing in AI image generation. Your task is to transform brief subject descriptions into detailed, high-quality prompts optimized for state-of-the-art image generation models like Flux, DALL-E 3, and Imagen.

For each theme, you must generate multiple distinct prompt variants that explore different artistic interpretations while maintaining the core subject matter.

Each expanded prompt MUST include:
1. **Composition details**: Camera angle, framing, perspective, focal point
2. **Lighting**: Type of lighting (natural, studio, dramatic, soft, etc.), time of day, light source direction
3. **Atmosphere/Mood**: Emotional tone, ambient feeling, weather/environmental conditions
4. **Texture and Materials**: Surface qualities, material descriptions, tactile details
5. **Style references**: Artistic style, medium (photography, painting, illustration), artistic movements
6. **Technical quality markers**: Resolution indicators, quality descriptors (8k, masterpiece, highly detailed)

Output Guidelines:
- Each variant should be distinct in style/interpretation (e.g., Realistic, Artistic, Cinematic, Abstract, Surreal)
- Prompts should be 50-150 words for optimal generation quality
- Include negative prompt suggestions for each variant
- Extract 3-5 relevant keywords for categorization
- Generate an English slug for file naming (lowercase, hyphens, max 50 chars)

IMPORTANT: Always respond with valid JSON matching the specified schema. Do not include any text outside the JSON structure.`

// ============================================
// TYPES
// ============================================

/**
 * Thinking level for Gemini 3 models
 * Controls the depth of internal reasoning before producing a response
 *
 * - MINIMAL: Matches "no thinking" for most queries; minimizes latency
 * - LOW: Minimizes latency and cost; best for simple tasks
 * - MEDIUM: Mid-level reasoning depth
 * - HIGH: Maximizes reasoning depth (default for Gemini 3)
 */
export type ThinkingLevel = 'MINIMAL' | 'LOW' | 'MEDIUM' | 'HIGH'

/**
 * Default thinking level for Gemini 3 models
 */
export const DEFAULT_THINKING_LEVEL: ThinkingLevel = 'LOW'

/**
 * Input for prompt expansion
 */
export interface PromptExpansionInput {
  /** The user's original subject description */
  subject: string
  /** Number of variants to generate (default: 3) */
  variantCount?: number
  /** Enable web search for trending topic context */
  webSearchEnabled?: boolean
}

/**
 * A single prompt variant generated by the LLM
 */
export interface PromptVariant {
  /** Unique identifier for this variant */
  variantId: string
  /** Human-readable name (e.g., "Realistic", "Artistic") */
  variantName: string
  /** The expanded, detailed prompt */
  expandedPrompt: string
  /** Suggested negative prompt for this variant */
  suggestedNegativePrompt: string
  /** Keywords extracted from the prompt for categorization */
  keywords: string[]
}

/**
 * Result of prompt expansion
 */
export interface PromptExpansionResult {
  /** Array of generated prompt variants */
  variants: PromptVariant[]
  /** English slug for file naming */
  subjectSlug: string
  /** Web search context if enabled */
  searchContext?: string
}

/**
 * LLM Provider interface for abstraction
 * Enables swapping between Gemini, ChatGPT, Claude, etc.
 */
export interface LLMProvider {
  /** Unique identifier for this provider */
  readonly providerId: string

  /**
   * Generate text from a prompt
   * @param prompt - The user's prompt/question
   * @param systemPrompt - System instructions for the LLM
   * @returns The generated text response
   */
  generate(prompt: string, systemPrompt: string): Promise<string>
}

// ============================================
// UTILITY FUNCTIONS
// ============================================

/**
 * Generate a URL-safe slug from the subject
 *
 * @param subject - The original subject text
 * @returns A lowercase, hyphenated slug (max 50 chars)
 */
export function generateSubjectSlug(subject: string): string {
  const slug = subject
    .toLowerCase()
    // Normalize unicode characters (remove accents)
    .normalize('NFD')
    .replace(/[\u0300-\u036f]/g, '')
    // Remove non-ASCII characters (CJK, emoji, etc.)
    .replace(/[^\x00-\x7F]/g, '')
    // Replace non-alphanumeric with hyphens
    .replace(/[^a-z0-9]+/g, '-')
    // Remove leading/trailing hyphens
    .replace(/^-+|-+$/g, '')
    // Truncate to 50 characters
    .substring(0, 50)

  return slug || 'untitled'
}

/**
 * Build the user prompt for LLM expansion
 */
function buildUserPrompt(input: PromptExpansionInput): string {
  const variantCount = input.variantCount ?? DEFAULT_VARIANT_COUNT

  let prompt = `Theme/Subject: "${input.subject}"

Generate ${variantCount} distinct prompt variants for this theme.`

  if (input.webSearchEnabled) {
    prompt += `

IMPORTANT: Use your knowledge of current trends and web search capabilities to enhance the prompts with trending aesthetics, popular styles, or current cultural references relevant to this theme.`
  }

  prompt += `

Respond with a JSON object matching this exact schema:
{
  "variants": [
    {
      "variantId": "variant-1",
      "variantName": "Realistic",
      "expandedPrompt": "detailed prompt here...",
      "suggestedNegativePrompt": "things to avoid...",
      "keywords": ["keyword1", "keyword2", "keyword3"]
    }
  ],
  "subjectSlug": "url-safe-slug"${input.webSearchEnabled ? ',\n  "searchContext": "brief description of trending context used"' : ''}
}`

  return prompt
}

/**
 * Parse and validate LLM response
 */
function parseExpansionResponse(responseText: string): PromptExpansionResult {
  if (!responseText || responseText.trim() === '') {
    throw new Error('Empty response from LLM')
  }

  let parsed: unknown
  try {
    // Try to extract JSON from the response (handle markdown code blocks)
    let jsonText = responseText.trim()
    if (jsonText.startsWith('```json')) {
      jsonText = jsonText.slice(7)
    }
    if (jsonText.startsWith('```')) {
      jsonText = jsonText.slice(3)
    }
    if (jsonText.endsWith('```')) {
      jsonText = jsonText.slice(0, -3)
    }
    jsonText = jsonText.trim()

    parsed = JSON.parse(jsonText)
  } catch {
    throw new Error(`Invalid JSON response from LLM: ${responseText.substring(0, 100)}...`)
  }

  // Validate structure
  const result = parsed as Record<string, unknown>

  if (!result.variants || !Array.isArray(result.variants)) {
    throw new Error('Response missing required "variants" array')
  }

  if (result.variants.length === 0) {
    throw new Error('Response contains empty variants array')
  }

  // Validate each variant
  for (const variant of result.variants) {
    const v = variant as Record<string, unknown>
    if (!v.variantId || typeof v.variantId !== 'string') {
      throw new Error('Variant missing required "variantId" field')
    }
    if (!v.variantName || typeof v.variantName !== 'string') {
      throw new Error('Variant missing required "variantName" field')
    }
    if (!v.expandedPrompt || typeof v.expandedPrompt !== 'string') {
      throw new Error('Variant missing required "expandedPrompt" field')
    }
  }

  return {
    variants: result.variants as PromptVariant[],
    subjectSlug: (result.subjectSlug as string) || generateSubjectSlug(String(result.subjectSlug || '')),
    searchContext: result.searchContext as string | undefined,
  }
}

// ============================================
// LLM PROVIDERS
// ============================================

/**
 * Default model for Gemini provider
 * Using Gemini 3 Flash Preview for improved reasoning performance
 */
export const DEFAULT_GEMINI_MODEL = 'gemini-3-flash-preview'

/**
 * Gemini 3 Flash Preview LLM Provider (Google AI)
 *
 * Primary provider for prompt optimization.
 * Uses the Gemini 3 Flash Preview model with JSON output mode.
 * Supports configurable thinking levels for latency vs reasoning depth tradeoffs.
 */
export class GeminiProvider implements LLMProvider {
  private client: GoogleGenerativeAI
  private modelName: string
  private thinkingLevel: ThinkingLevel

  /**
   * Create a new GeminiProvider instance
   *
   * @param apiKey - Google AI API key
   * @param modelName - Model name (default: gemini-3-flash-preview)
   * @param thinkingLevel - Thinking level for Gemini 3 models (default: LOW)
   */
  constructor(
    apiKey: string,
    modelName: string = DEFAULT_GEMINI_MODEL,
    thinkingLevel: ThinkingLevel = DEFAULT_THINKING_LEVEL
  ) {
    this.client = new GoogleGenerativeAI(apiKey)
    this.modelName = modelName
    this.thinkingLevel = thinkingLevel
  }

  get providerId(): string {
    return 'gemini'
  }

  /**
   * Get the current model name
   */
  getModelName(): string {
    return this.modelName
  }

  /**
   * Get the current thinking level
   */
  getThinkingLevel(): ThinkingLevel {
    return this.thinkingLevel
  }

  async generate(prompt: string, systemPrompt: string): Promise<string> {
    // Build generation config based on model type
    const isGemini3 = this.modelName.includes('gemini-3')

    const generationConfig: Record<string, unknown> = {
      responseMimeType: 'application/json',
    }

    // Add thinkingConfig for Gemini 3 models
    if (isGemini3) {
      generationConfig.thinkingConfig = {
        thinkingLevel: this.thinkingLevel,
      }
    }

    const model = this.client.getGenerativeModel({
      model: this.modelName,
      generationConfig,
    })

    // Combine system prompt and user prompt
    const fullPrompt = `${systemPrompt}\n\n---\n\n${prompt}`

    const result = await model.generateContent(fullPrompt)
    const response = result.response
    const text = response.text()

    return text
  }
}

/**
 * Placeholder for future ChatGPT provider
 * Will implement GPT-4o support
 */
export class ChatGPTProvider implements LLMProvider {
  get providerId(): string {
    return 'chatgpt'
  }

  async generate(_prompt: string, _systemPrompt: string): Promise<string> {
    throw new Error('ChatGPT provider not yet implemented. Use GeminiProvider.')
  }
}

/**
 * Placeholder for future Claude provider
 * Will implement Claude 3.5 Sonnet support
 */
export class ClaudeProvider implements LLMProvider {
  get providerId(): string {
    return 'claude'
  }

  async generate(_prompt: string, _systemPrompt: string): Promise<string> {
    throw new Error('Claude provider not yet implemented. Use GeminiProvider.')
  }
}

// ============================================
// PROMPT OPTIMIZER
// ============================================

/**
 * Prompt Optimizer Service
 *
 * Main service class for intelligent prompt optimization.
 * Uses an LLM provider to transform brief themes into detailed prompts.
 */
export class PromptOptimizer {
  private provider: LLMProvider

  /**
   * Create a new PromptOptimizer instance
   *
   * @param provider - The LLM provider to use for generation
   */
  constructor(provider: LLMProvider) {
    this.provider = provider
  }

  /**
   * Get the ID of the current LLM provider
   */
  getProviderId(): string {
    return this.provider.providerId
  }

  /**
   * Expand a brief subject description into detailed prompt variants
   *
   * @param input - The expansion input containing subject and options
   * @returns The expansion result with variants and metadata
   */
  async expandPrompt(input: PromptExpansionInput): Promise<PromptExpansionResult> {
    // Build the user prompt
    const userPrompt = buildUserPrompt(input)

    // Call the LLM
    const responseText = await this.provider.generate(userPrompt, SYSTEM_PROMPT)

    // Parse and validate the response
    const result = parseExpansionResponse(responseText)

    return result
  }
}

// ============================================
// FACTORY FUNCTIONS
// ============================================

/**
 * Options for creating a PromptOptimizer
 */
export interface CreatePromptOptimizerOptions {
  /** Google AI API key (defaults to GOOGLE_AI_API_KEY env var) */
  apiKey?: string
  /** Model name (defaults to gemini-3-flash-preview) */
  modelName?: string
  /** Thinking level for Gemini 3 models (defaults to LOW) */
  thinkingLevel?: ThinkingLevel
}

/**
 * Create a default PromptOptimizer using Gemini 3 Flash Preview
 *
 * @param options - Configuration options
 * @returns A configured PromptOptimizer instance
 *
 * @example
 * ```ts
 * // Using defaults (gemini-3-flash-preview with LOW thinking)
 * const optimizer = createPromptOptimizer()
 *
 * // With custom model and thinking level
 * const optimizer = createPromptOptimizer({
 *   modelName: 'gemini-3-flash-preview',
 *   thinkingLevel: 'MEDIUM'
 * })
 * ```
 */
export function createPromptOptimizer(options?: CreatePromptOptimizerOptions): PromptOptimizer {
  const key = options?.apiKey || process.env.GOOGLE_AI_API_KEY
  if (!key) {
    throw new Error('GOOGLE_AI_API_KEY environment variable is required')
  }

  const provider = new GeminiProvider(
    key,
    options?.modelName ?? DEFAULT_GEMINI_MODEL,
    options?.thinkingLevel ?? DEFAULT_THINKING_LEVEL
  )
  return new PromptOptimizer(provider)
}

/**
 * Create a PromptOptimizer with a custom provider
 *
 * @param provider - The LLM provider to use
 * @returns A configured PromptOptimizer instance
 */
export function createPromptOptimizerWithProvider(provider: LLMProvider): PromptOptimizer {
  return new PromptOptimizer(provider)
}
